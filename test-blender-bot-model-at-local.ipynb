{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANIC SERVICE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text_obj = {'text': 'hello there, how are you doing?'}\n",
    "stt_server = 'http://127.0.0.1:8000/open-chat'\n",
    "\n",
    "# text = requests.post(stt_server, data=json.dumps(text_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\" I'm sorry to hear that. What are you going to do to make up for it?\"}\n"
     ]
    }
   ],
   "source": [
    "text_obj = {'text': \"yeah, i'm having a deadline tomorrow, so i'm rushing now :( quite exhausting to be honest\"}\n",
    "text = requests.post(stt_server, data=json.dumps(text_obj))\n",
    "print(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON-HOSTING TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "processor       : 0\n",
      "vendor_id       : GenuineIntel\n",
      "cpu family      : 6\n",
      "model           : 42\n",
      "model name      : Intel(R) Core(TM) i3-2330M CPU @ 2.20GHz\n",
      "stepping        : 7\n",
      "microcode       : 0x2f\n",
      "cpu MHz         : 2000.000\n",
      "cache size      : 3072 KB\n",
      "physical id     : 0\n",
      "siblings        : 4\n",
      "core id         : 0\n",
      "cpu cores       : 2\n",
      "apicid          : 0\n",
      "initial apicid  : 0\n",
      "fpu             : yes\n",
      "fpu_exception   : yes\n",
      "cpuid level     : 13\n",
      "wp              : yes\n",
      "\u001b[K:\u001b[K         : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1\u001b[7m/proc/cpuinfo\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "!less /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Facebook-Blender-Bot/bbot-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-05-05 09:31:28.690529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-05 09:31:28.690590: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TOKENIZER took:  0.15622591972351074\n",
      "\n",
      "MODEL generate took:  24.902355909347534\n",
      "\n",
      "TOKNEIZER batch decode took:  0.0029723644256591797\n",
      "Bot:   It is a little chilly, but not too bad. How is it where you are?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "UTTERANCE = \"hello there, how is the weather today?\"\n",
    "\n",
    "start = time.time()\n",
    "tokenized_text = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "end = time.time()\n",
    "print(\"\\n\\nTOKENIZER took: \", end - start)\n",
    "\n",
    "# Perform translation and decode the output\n",
    "start = time.time()\n",
    "reply_ids = model.generate(**tokenized_text)\n",
    "end = time.time()\n",
    "print(\"\\nMODEL generate took: \", end - start)\n",
    "\n",
    "start = time.time()\n",
    "reply_text = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "end = time.time()\n",
    "print(\"\\nTOKNEIZER batch decode took: \", end - start)\n",
    "\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cbd5d387a4d8a6421a29b89b8806734114d39b58a9e16f7b2b8eb9485e169e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('bbot-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
